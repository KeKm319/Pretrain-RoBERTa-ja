{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scr.roberta import Model\n",
    "from scr.tokenizer import Tokenizer\n",
    "from scr.datasets import Roberta_datasets\n",
    "from scr.utils import get_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "\n",
    "torch_fix_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embeddings(\n",
       "    (word_embs): Embedding(1500, 384, padding_idx=3)\n",
       "    (pos_enc): Embedding(129, 384)\n",
       "    (seg_emb): Embedding(2, 384)\n",
       "    (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (roberta): Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x Layer(\n",
       "        (attention): Attention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (MHA): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): Intermediate(\n",
       "          (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act_fn): GELU(approximate='none')\n",
       "          (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlm): MLM(\n",
       "    (out_linear): Linear(in_features=384, out_features=1500, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer('tokenizer/tokenizer.model')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = 128\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'max_len': max_len,\n",
    "    'num_layers': 6,\n",
    "    'num_attn_heads': 4,\n",
    "    'hidden_dim': 384,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "num_epoch = 5\n",
    "PAD = tokenizer.label_2_id('[PAD]')\n",
    "MASK = tokenizer.label_2_id('[MASK]')\n",
    "model = Model(**config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]メロスは激怒した。[EOS]', '[CLS]必ず、かの邪智暴虐の王を除かなければならぬと決意した。[EOS]', '[CLS]メロスには政治がわからぬ。[EOS]', '[CLS]メロスは、村の牧人である。[EOS]', '[CLS]笛を吹き、羊と遊んで暮して来た。[EOS]']\n",
      "[[4, 16, 652, 18, 739, 8], [4, 624, 738, 480, 680, 897, 1196, 740, 794, 746, 0, 471, 752, 649, 18, 739, 8], [4, 462, 1363, 1394, 759, 454, 739, 8], [4, 16, 738, 373, 380, 57, 739, 8], [4, 1445, 746, 1267, 765, 738, 956, 752, 0, 100, 949, 446, 739, 8]]\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/text.txt', 'r', encoding='utf-8')as f:\n",
    "    datas = f.read()\n",
    "datas = datas.split('\\n')\n",
    "print(datas[:5])\n",
    "datas = tokenizer.encode_texts(datas)\n",
    "print(datas[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(datas, test_size=0.1, random_state=42)\n",
    "train_data = Roberta_datasets(train, tokenizer, max_len,vocab_size, PAD, MASK)\n",
    "test_data = Roberta_datasets(test, tokenizer, max_len, vocab_size, PAD, MASK)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "special_token_weight = torch.tensor([1.0]*vocab_size)\n",
    "special_token_weight[:9] = 0.0\n",
    "special_token_weight = special_token_weight.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(get_trainable_parameters(model), lr=learning_rate, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0005)\n",
    "criterion = nn.NLLLoss(weight=special_token_weight)\n",
    "log_softmax = nn.LogSoftmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c968ec4dfd432688d6f642678570cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/5, train_loss : 6.839238779885428, val_loss : 6.441524982452393\n",
      "epoch : 2/5, train_loss : 6.416412217276437, val_loss : 5.995678901672363\n",
      "epoch : 3/5, train_loss : 6.395083972385952, val_loss : 6.229027271270752\n",
      "epoch : 4/5, train_loss : 6.236432347978864, val_loss : 6.7290449142456055\n",
      "epoch : 5/5, train_loss : 6.3231392587934225, val_loss : 6.299596309661865\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(num_epoch)):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    model.train()\n",
    "    for x, pos, token_ids, mask, mask_labels in train_dataloader:\n",
    "        output = model.forward(x, pos, token_ids, mask)\n",
    "        output = log_softmax(output)\n",
    "        loss = criterion(output.transpose(1, 2), mask_labels)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, pos, token_ids, mask, mask_labels in test_dataloader:\n",
    "            output = model.forward(x, pos, token_ids, mask)\n",
    "            output = log_softmax(output)\n",
    "            loss = criterion(output.transpose(1, 2), mask_labels)\n",
    "            val_loss += loss.item()\n",
    "    print(f'epoch : {e+1}/{num_epoch}, train_loss : {train_loss/len(train_dataloader)}, val_loss : {val_loss/len(test_dataloader)}')\n",
    "torch.save(model.state_dict(), 'model/roberta_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'メロスは', '[MASK]', 'した', '。', '[EOS]']\n",
      "[MASK] is \n",
      "top 1 : 、\n",
      "top 2 : 。\n",
      "top 3 : 「\n",
      "top 4 : の\n",
      "top 5 : を\n",
      "top 6 : に\n",
      "top 7 : 。」\n",
      "top 8 : は\n",
      "top 9 : も\n",
      "top 10 : メロスは\n"
     ]
    }
   ],
   "source": [
    "from scr.utils import make_input\n",
    "model.eval()\n",
    "text = '[CLS]メロスは[MASK]した。[EOS]'\n",
    "print(tokenizer.text_2_token(text))\n",
    "x, pos, token_ids, mask, mask_idx = make_input(text, tokenizer)\n",
    "with torch.no_grad():\n",
    "    out = model.forward(x, pos, token_ids, mask)\n",
    "print('[MASK] is ')\n",
    "for i, t in enumerate(out[0][mask_idx].topk(10)[1]):\n",
    "    pred = tokenizer.id_2_label(t.item())\n",
    "    print(f'top {i+1} : {pred}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
